<!DOCTYPE html>
<html>

<head>
    <title>CAST:Cross-Attention in Space and Time for Video Action Recognition</title>
    <link rel="stylesheet" type="text/css" href="styles.css"> <!-- CSS 파일을 연결합니다 -->
    <meta name="author" content="Dongho Lee, Jongseo Lee, Jinwoo Choi">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

</head>

<body>
    <section id="home">
    </section>
    <header class="red-bar">
        <nav>
            <ul>
                <li><a href="#home">Home</a></li>
                <li><a href="#abstract">Abstract</a></li>
                <li><a href="#introduction">Introduction</a></li>
                <li><a href="#method">Method</a></li>
                <li><a href="#results">Result</a></li>
                <li><a href="#demo">Demo</a></li>
                <li><a href="#download">Download</a></li>
                <li><a href="#citation">Citation</a></li>
                <li><a href="#references">References</a></li>
                <!-- <figure>
                    <img src="figure/LOGO.png" alt="introduction_figure" class="introduction">
                </figure> -->
            </ul>
        </nav>
    </header>


    <div class="author_information">

        <div class="container"></div>
        <section id="paper">
            <h2>CAST:<br>Cross-Attention in Space and Time<br>for Video Action Recognition</h2>
            <!-- 논문 내용을 이어서 작성합니다 -->
        </section>
        <section id="conference">
            <h3>In Neural Information Processing Systems (NeurIPS 2023), New Orleans, USA</h3>
        </section>
        <p class="authors">
            <a href="https://github.com/potatowarriors/" id="dongho">Dongho Lee<sup>†</sup></a>,
            <a href="https://github.com/jong980812" id="jongseo">Jongseo Lee<sup>†</sup></a>,
            <a href="https://sites.google.com/site/jchoivision/" id="jinwoo">Jinwoo Choi<sup>*</sup></a>,

        </p>
        <p class="authors">
            <a href="https://www.khu.ac.kr/eng/main/index.do">Kyung Hee University <sup></sup></a>
            
        </p>

        <section id="corresponding">
            <sup>*</sup>Corresponding author<br>
            <sup style="color: black;">†</sup>Equally contributed
            <!-- 논문 내용을 이어서 작성합니다 -->
        </section>

    </div>

    <!-- Abstract 문단 추가 -->
    <section id="Home_link">
        <ul>
            <a href="https://github.com/KHU-VLL/CAST"><i class="fa fa-github"></i> GitHub</a>
            <a href="https://vll.khu.ac.kr/"><i class="fa fa-globe"></i> Lab homepage</a>
        </ul>
    </section>
    <section id="abstract">
        <h1>Abstract</h1>
        <p>Recognizing human actions in videos requires spatial and temporal understanding.
            Most existing action recognition models lack a balanced spatio-temporal understanding of videos.

            In this work, we propose a novel two-stream architecture, called Cross-Attention in Space and Time
            (CAST), that achieves a balanced spatio-temporal understanding of videos using only RGB input.

            Our proposed bottleneck cross-attention mechanism enables the spatial and temporal expert models to
            exchange information and make synergistic predictions, leading to improved performance.

            We validate the proposed method with extensive experiments on public benchmarks with different
            characteristics: EPIC-KITCHENS-100, Something-Something-V2, and Kinetics-400.

            Our method consistently shows favorable performance across these datasets, while the performance of
            existing methods fluctuates depending on the dataset characteristics.
        </p>
    </section>
    <section id="introduction">
        <h1>Introduction</h1>

        <!-- 첫 번째 그림 및 내용 -->
        <!-- 그림 추가: 이미지 파일 경로를 src에 설정 -->
        <figure>
            <img src="figure/introduction.jpg" alt="introduction_figure" class="introduction">
            <figcaption><strong>The importance of spatio-temporal understanding.</strong></figcaption>
        </figure>
        <p>To accurately recognize human actions in videos, a model must understand both the spatial and temporal
            contexts. A model that lacks fine-grained spatial understanding is likely to fail in predicting the correct
            action.
            a model that understands temporal context such as hand motion across frames but not the fine-grained spatial
            context may confuse whether an object in the hand a <em>ketchup</em>, or a <em>cheese</em>, or a <em>Milk
                carton</em>
            Consequently, the model fails to predict the correct action, <em>Put down a cheese</em>. Similarly, a
            model that lacks temporal context understanding may also fail to predict the correct action.
            Let us suppose a model understands spatial context but does not understand temporal
            context, <em>e.g.</em>,the model is confused about whether the hand is moving from outside the fridge to the
            inside or
            vice versa. Then the model fails to predict the correct action of <em>Take out a sauce</em>. Therefore, for
            accurate action recognition, models need to comprehend both the spatial and temporal contexts of videos.
        </p>
        <figure>
            <img src="figure/motivation.jpg" alt="motivation_figure" class="motivation">
            <figcaption><strong>High-level illustration of the proposed method.</strong></figcaption>
        </figure>

        <p>we show a high-level illustration of the proposed method. Our architecture employs two expert models - a
            <span class="red-text">spatial</span> expert model and a <span class="blue-text">temporal </span>expert
            model - which exchange information to make a synergistic
            collective prediction.
            Initially, the experts may predict incorrect actions due to the lack of information. For example, the
            temporal expert may predict reach out to something while the ground truth is Pick up a fork. Similarly,
            the spatial expert may predict utensil holder instead of fork in the shallower layers. However, after
            using cross-attention to exchange information multiple times, the proposed method can collectively
            predict the correct action Pick up a fork. Best viewed with zoom and color
        </p>
    </section>
    <section id="method">
        <h1>Method</h1>

        <!-- 첫 번째 그림 및 내용 -->
        <!-- 그림 추가: 이미지 파일 경로를 src에 설정 -->
        <figure>
            <img src="./figure/model.jpg" alt="model_figure" class="model">
             <figcaption><strong> Overview of CAST.</strong></figcaption> 
            <!-- <video width="900" autoplay loop> -->
                <!-- <source src="figure/model_animation.mp4" type="video/mp4"> -->
                <!-- Your browser does not support the video tag. -->
            <!-- </video> -->
            
        </figure>
        <p>We introduce CAST, a method for balanced spatio-temporal representation learning for action recognition, as
            shown in Figure. We employ frozen spatial and temporal expert models that can be any
            vision transformer, consisting of 12 transformer blocks each. To facilitate information exchange between the
            experts, we introduce the bottleneck cross-attention in space and time (B-CAST) module on top of the frozen
            layers. This module enables the experts to exchange information and learn more balanced spatio-temporal
            contexts than separate experts. To improve adaptation to downstream tasks, we use adapter layers with a
            small number of learnable parameters, following AIM<sup>[1]</sup>.</p>
    </section>


    <section id="results">
        <h1>Results</h1>

        <figure>
            <figcaption><strong> Comparison with the state-of-the-arts on the EPIC-Kitchens-100 dataset.</strong></figcaption>
            <img src="table/EK100.png" alt="EK100" class="model">
        </figure>
        <figure>
            <figcaption><strong> Comparison with the state-of-the-arts on the Kinetics400 dataset.</strong></figcaption>
            <img src="table/K400.png" alt="K400" class="model">
        </figure>
        <figure>
            <figcaption><strong> Comparison with the state-of-the-arts on the Something-Something-V2 dataset.</strong></figcaption>
            <img src="table/SSV2.png" alt="SSV2" class="model">
        </figure>
        <figure>
            <figcaption><strong>Qualitative examples from EK100 comparing CLIP, VideoMAE, and the proposed
                CAST.</strong></figcaption>
            <img src="figure/failure.jpg" alt="failure" class="failure">
        </figure>
    </section>

    <!-- <section id="links">
        <h1>Links</h1>
        <ul>
            <li><a href="https://github.com/khuvll/CAST"><i class="fa fa-github"></i> GitHub</a></li>
            <li><a href="https://vll.khu.ac.kr/"><i class="fa fa-globe"></i> Lab homepage</a></li>
            <li>
                <i class="fa fa-book"></i> Paper download
                <ul>
                    <li><a href="#"><i class="fa fa-download"></i> Neruips version</a></li>
                    <li><a href="#"><i class="fa fa-download"></i> Arxiv version</a></li>
                </ul>
            </li>
        </ul>
    </section> -->
    <section id="download">
        <h1>Download</h1>
        <p style="font-family: monospace; font-size: 1em; white-space: pre;">
        </p>
    </section>
    
  <section id="citation">
    <h1>Citation</h1>
    <p style="font-family: monospace; font-size: 1em; white-space: pre;">
        @article{cast,
        &emsp; title={CAST: Cross-Attention in Space and Time for Video Action Recognition},
        &emsp; author={Lee, Dongho and Lee, Jongseo and Choi, Jinwoo},
        &emsp; booktitle={NeurIPS},
        &emsp; year={2023}
        }
    </p>
</section>

    <section id="references">
        <h1>References</h1>
        <p>[1] Yang, Taojiannan, et al. "AIM: Adapting Image Models for Efficient Video Action Recognition." The Eleventh International Conference on Learning Representations. 2022.</p>
    </section>

    

    
</body>

</html>