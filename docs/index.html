<!DOCTYPE html>
<html>

<head>
    <title>Your Research Paper</title>
    <link rel="stylesheet" type="text/css" href="styles.css"> <!-- CSS 파일을 연결합니다 -->
    <meta name="author" content="Taojiannan Yang, Yi Zhu, Yusheng Xie, Aston Zhang, Chen Chen, Mu Li">
</head>

<body>

    <header class="red-bar">
        <nav>
            <ul>
                <li><a href="#home">Home</a></li>
                <li><a href="#abstract">Abstract</a></li>
                <li><a href="#method">Method</a></li>
                <li><a href="#result">Result</a></li>
                <li><a href="#demo">Demo</a></li>
                <li><a href="#paper">Paper</a></li>
                <li><a href="#references">References</a></li>
                <li><a href="#citation">Citation</a></li>
            </ul>
        </nav>
    </header>


    <div class="author_information">

        <div class="container"></div>
        <section id="paper">
            <h2>CAST<br>:Cross-Attention in Space and Time for Video Action Recognition</h2>
            <!-- 논문 내용을 이어서 작성합니다 -->
        </section>

        <p class="authors">
            <a href="">Dongho Lee<sup></sup></a>,
            <a href="https://github.com/jong980812">Jongseo Lee<sup></sup></a>,
            <a href="https://sites.google.com/site/jchoivision/">Jinwoo Choi<sup>*</sup></a>,

        </p>
        <p class="authors">
            <a href="https://www.khu.ac.kr/eng/main/index.do">Kyung Hee University<sup></sup></a>
        </p>
        
        <section id="corresponding">
            <sup>*</sup>Corresponding author
            <!-- 논문 내용을 이어서 작성합니다 -->
        </section>
    </div>
        <!-- Abstract 문단 추가 -->
        <section id="abstract">
            <h1>Abstract</h1>
            <p>Recognizing human actions in videos requires spatial and temporal understanding.
                Most existing action recognition models lack a balanced spatio-temporal understanding of videos.

                In this work, we propose a novel two-stream architecture, called Cross-Attention in Space and Time
                (CAST), that achieves a balanced spatio-temporal understanding of videos using only RGB input.

                Our proposed bottleneck cross-attention mechanism enables the spatial and temporal expert models to
                exchange information and make synergistic predictions, leading to improved performance.

                We validate the proposed method with extensive experiments on public benchmarks with different
                characteristics: EPIC-KITCHENS-100, Something-Something-V2, and Kinetics-400.

                Our method consistently shows favorable performance across these datasets, while the performance of
                existing methods fluctuates depending on the dataset characteristics.
            </p>
        </section>



</body>

</html>